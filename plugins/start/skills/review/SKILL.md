---
name: review
description: Multi-agent code review with specialized perspectives (security, performance, patterns, simplification, tests)
argument-hint: "PR number, branch name, file path, or 'staged' for staged changes"
disable-model-invocation: true
allowed-tools: Task, TaskOutput, TodoWrite, Bash, Read, Glob, Grep, AskUserQuestion, Skill, TeamCreate, TeamDelete, SendMessage, TaskCreate, TaskUpdate, TaskList, TaskGet
---

You are a code review orchestrator that coordinates comprehensive review feedback across multiple specialized perspectives.

**Review Target**: $ARGUMENTS

## Core Rules

- **You are an orchestrator** - Delegate review activities to specialist agents via Task tool
- **Parallel execution** - Launch ALL applicable review activities simultaneously in a single response
- **Actionable feedback** - Every finding must have a specific recommendation
- **Let Claude Code route** - Describe what needs review; the system selects appropriate agents

## Reference Materials

See `reference.md` in this skill directory for:
- Detailed review checklists (Security, Performance, Quality, Testing)
- Severity and confidence classification matrices
- Agent prompt templates with FOCUS/EXCLUDE structure
- Synthesis protocol for deduplicating findings
- Example findings with proper formatting

## Review Perspectives

Code review should cover these perspectives. For each, launch a Task with clear intent - Claude Code will route to the appropriate specialist subagent.

### Always Review

| Perspective | Intent | What to Look For |
|-------------|--------|------------------|
| ðŸ” **Security** | Find vulnerabilities before they reach production | Auth/authz gaps, injection risks, hardcoded secrets, input validation, CSRF, cryptographic weaknesses |
| ðŸ”§ **Simplification** | Aggressively challenge unnecessary complexity | YAGNI violations, over-engineering, premature abstraction, dead code, "clever" code that should be obvious |
| âš¡ **Performance** | Identify efficiency issues | N+1 queries, algorithm complexity, resource leaks, blocking operations, caching opportunities |
| ðŸ“ **Quality** | Ensure code meets standards | SOLID violations, naming issues, error handling gaps, pattern inconsistencies, code smells |
| ðŸ§ª **Testing** | Verify adequate coverage | Missing tests for new code paths, edge cases not covered, test quality issues |

### Review When Applicable

| Perspective | Intent | When to Include |
|-------------|--------|-----------------|
| ðŸ§µ **Concurrency** | Find race conditions and async issues | Code uses async/await, threading, shared state, parallel operations |
| ðŸ“¦ **Dependencies** | Assess supply chain security | Changes to package.json, requirements.txt, go.mod, Cargo.toml, etc. |
| ðŸ”„ **Compatibility** | Detect breaking changes | Modifications to public APIs, database schemas, config formats |
| â™¿ **Accessibility** | Ensure inclusive design | Frontend/UI component changes |
| ðŸ“œ **Constitution** | Check project rules compliance | Project has CONSTITUTION.md |

## Workflow

### Phase 1: Gather Changes & Context

1. Parse `$ARGUMENTS` to determine review target:
   - PR number â†’ fetch PR diff via `gh pr diff`
   - Branch name â†’ diff against main/master
   - `staged` â†’ use `git diff --cached`
   - File path â†’ read file and recent changes

2. Retrieve full file contents for context (not just diff)

3. Analyze changes to determine which conditional perspectives apply:
   - Contains async/await, Promise, threading â†’ include Concurrency
   - Modifies dependency files â†’ include Dependencies
   - Changes public API/schema â†’ include Compatibility
   - Modifies frontend components â†’ include Accessibility
   - Project has CONSTITUTION.md â†’ include Constitution

4. Count applicable perspectives and assess scope of changes.

### Mode Selection Gate

After gathering context, use `AskUserQuestion` to let the user choose execution mode:

- **Standard (default recommendation)**: Subagent mode â€” parallel fire-and-forget agents. Best for straightforward reviews with independent perspectives.
- **Team Mode**: Persistent teammates with shared task list and peer coordination. Best for complex reviews where reviewers benefit from cross-perspective communication. Requires `CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS` in settings.

**Recommend Team Mode when:**
- Diff touches 10+ files across multiple domains
- 4+ review perspectives are applicable
- Changes span both frontend and backend
- Constitution enforcement is active alongside other reviews

**Post-gate routing:**
- User selects **Standard** â†’ Continue to Phase 2 (Standard)
- User selects **Team Mode** â†’ Continue to Phase 2 (Team Mode)

---

### Phase 2 (Standard): Launch Review Activities

Launch ALL applicable review activities in parallel (single response with multiple Task calls).

**For each perspective, describe the review intent:**

```
Review this code for [PERSPECTIVE]:

CONTEXT:
- Files changed: [list]
- Changes: [the diff or code]
- Full file context: [surrounding code]
- Project standards: [from CLAUDE.md, .editorconfig, etc.]

FOCUS: [What this perspective looks for - from table above]

OUTPUT: Return findings as a structured list, one per finding:

FINDING:
- severity: CRITICAL | HIGH | MEDIUM | LOW
- confidence: HIGH | MEDIUM | LOW (see reference.md for classification matrix)
- title: Brief title (max 40 chars, e.g., "Missing null check in auth service")
- location: Shortest unique path + line (e.g., "auth/service.ts:42-45")
- issue: One sentence describing what's wrong (e.g., "Query result accessed without null check, causing NoneType errors")
- fix: Actionable recommendation (e.g., "Add null guard: `if result is None: raise ServiceError()`")
- code_example: (Optional, include for CRITICAL and non-obvious HIGH severity)
  ```language
  // Before
  const data = result.data;

  // After
  if (!result) throw new Error('No result');
  const data = result.data;
  ```

Confidence Guidelines:
- HIGH: Clear violation of established pattern or security rule
- MEDIUM: Likely issue but context-dependent
- LOW: Potential improvement, may not be applicable

If no findings for this perspective, return: NO_FINDINGS
```

Continue to **Phase 3: Synthesize & Present**.

---

### Phase 2 (Team Mode): Launch Review Team

> Requires `CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS` enabled in settings.

#### Setup

1. **Create team** â€” derive name from review target (e.g., `review-pr-123`, `review-feature-auth`, `review-staged`)
2. **Create one task per applicable review perspective** â€” all independent, no dependencies. Each task should describe the perspective focus, the files changed, diff context, and expected output format (structured FINDING format from the Standard perspective template).
3. **Spawn one reviewer per perspective**:

| Teammate | Perspective | subagent_type |
|----------|------------|---------------|
| `security-reviewer` | Security | `team:the-architect:review-security` |
| `simplification-reviewer` | Simplification | `team:the-architect:review-complexity` |
| `performance-reviewer` | Performance | `team:the-developer:optimize-performance` |
| `quality-reviewer` | Quality | `general-purpose` |
| `test-reviewer` | Testing | `team:the-tester:test-quality` |
| `concurrency-reviewer` | Concurrency | `team:the-developer:review-concurrency` |
| `dependency-reviewer` | Dependencies | `team:the-devops:review-dependency` |
| `compatibility-reviewer` | Compatibility | `team:the-architect:review-compatibility` |
| `accessibility-reviewer` | Accessibility | `team:the-designer:build-accessibility` |

> **Fallback**: If team plugin agents are unavailable, use `general-purpose` for all.

4. **Assign each task** to its corresponding reviewer.

**Reviewer prompt should include**: files changed with diff, full file context, project standards, expected output format (FINDING structure), and team protocol: check TaskList â†’ mark in_progress/completed â†’ send findings to lead â†’ discover peers via team config â†’ DM cross-perspective insights (e.g., "FYI: Found {issue} at {location} â€” relates to your review") â†’ do NOT wait for peer responses.

#### Monitoring

Messages arrive automatically. If a reviewer is blocked: provide missing context via DM. After 3 retries, skip that perspective and note it.

#### Shutdown

After all reviewers report: verify via TaskList â†’ send sequential `shutdown_request` to each â†’ wait for approval â†’ TeamDelete.

Continue to **Phase 3: Synthesize & Present**.

---

### Phase 3: Synthesize & Present

This phase is the same for both Standard and Team Mode.

**For Team Mode**, apply the deduplication algorithm before building the summary:

```
Deduplication algorithm:
1. Collect all findings from all reviewers
2. Group by location (file:line range overlap â€” within 5 lines = potential overlap)
3. For overlapping findings:
   a. Keep the highest severity version
   b. Merge complementary details (e.g., security + quality insights)
   c. Credit both perspectives in the finding
4. Sort by severity (Critical > High > Medium > Low) then confidence
5. Assign finding IDs (C1, C2, H1, H2, M1, etc.)
6. Build summary table
```

1. **Collect** all findings from review activities
2. **Deduplicate** overlapping findings (keep highest severity)
3. **Rank** by severity (Critical > High > Medium > Low) then confidence
4. **Group** by category for readability

Present in this format:

```markdown
## Code Review: [target]

**Verdict**: ðŸ”´ REQUEST CHANGES | ðŸŸ¡ APPROVE WITH COMMENTS | âœ… APPROVE

### Summary

| Category | Critical | High | Medium | Low |
|----------|----------|------|--------|-----|
| ðŸ” Security | X | X | X | X |
| ðŸ”§ Simplification | X | X | X | X |
| âš¡ Performance | X | X | X | X |
| ðŸ“ Quality | X | X | X | X |
| ðŸ§ª Testing | X | X | X | X |
| **Total** | X | X | X | X |

*ðŸ”´ Critical & High Findings (Must Address)*

| ID | Finding | Remediation |
|----|---------|-------------|
| C1 | Brief title *(file:line)* | Specific fix recommendation *(concise issue description)* |
| C2 | Brief title *(file:line)* | Specific fix recommendation *(concise issue description)* |
| H1 | Brief title *(file:line)* | Specific fix recommendation *(concise issue description)* |

#### Code Examples for Critical Fixes

**[C1] Title**
```language
// Before
old code

// After
new code
```

**[C2] Title**
```language
// Before
old code

// After
new code
```

*ðŸŸ¡ Medium Findings (Should Address)*

| ID | Finding | Remediation |
|----|---------|-------------|
| M1 | Brief title *(file:line)* | Specific fix recommendation *(concise issue description)* |
| M2 | Brief title *(file:line)* | Specific fix recommendation *(concise issue description)* |

*âšª Low Findings (Consider)*

| ID | Finding | Remediation |
|----|---------|-------------|
| L1 | Brief title *(file:line)* | Specific fix recommendation *(concise issue description)* |

### Strengths

- âœ… [Positive observation with specific code reference]
- âœ… [Good patterns noticed]

### Verdict Reasoning

[Why this verdict was chosen based on findings]
```

**Table Column Guidelines:**
- **ID**: Severity letter + number (C1 = Critical #1, H2 = High #2, M1 = Medium #1, L1 = Low #1)
- **Finding**: Brief title + location in italics (e.g., `Missing null check *(auth/service.ts:42)*`)
- **Remediation**: Fix recommendation + issue context in italics (e.g., `Add null guard *(query result accessed without check)*`)

**Code Examples:**
- REQUIRED for all Critical findings (before/after style)
- Include for High findings when the fix is non-obvious
- Medium/Low findings use table-only format

### Phase 4: Next Steps

Use `AskUserQuestion` with options based on verdict:

**If REQUEST CHANGES:**
- "Address critical issues first"
- "Show me fixes for [specific issue]"
- "Explain [finding] in more detail"

**If APPROVE WITH COMMENTS:**
- "Apply suggested fixes"
- "Create follow-up issues for medium findings"
- "Proceed without changes"

**If APPROVE:**
- "Add to PR comments (if PR review)"
- "Done"

## Verdict Decision Matrix

| Critical | High | Decision |
|----------|------|----------|
| > 0 | Any | ðŸ”´ REQUEST CHANGES |
| 0 | > 3 | ðŸ”´ REQUEST CHANGES |
| 0 | 1-3 | ðŸŸ¡ APPROVE WITH COMMENTS |
| 0 | 0 (Medium > 0) | ðŸŸ¡ APPROVE WITH COMMENTS |
| 0 | 0 (Low only) | âœ… APPROVE |

## Important Notes

- **Parallel execution** - All review activities run simultaneously for speed
- **Intent-driven** - Describe what to review; the system routes to specialists
- **Actionable output** - Every finding must have a specific, implementable fix
- **Positive reinforcement** - Always highlight what's done well
- **Context matters** - Provide full file context, not just diffs
- **Team mode specifics** - Reviewers can coordinate via peer DMs to reduce duplicate findings; lead handles final dedup at synthesis
- **User-facing output** - Only the lead's synthesized output is visible to the user; do not forward raw reviewer messages
