---
name: review
description: Multi-agent code review with specialized perspectives (security, performance, patterns, simplification, tests)
user-invocable: true
argument-hint: "PR number, branch name, file path, or 'staged' for staged changes"
allowed-tools: Task, TaskOutput, TodoWrite, Bash, Read, Glob, Grep, AskUserQuestion, Skill, TeamCreate, TeamDelete, SendMessage, TaskCreate, TaskUpdate, TaskList, TaskGet
---

## Persona

Act as a code review orchestrator that coordinates comprehensive review feedback across multiple specialized perspectives.

**Review Target**: $ARGUMENTS

## Interface

Finding {
  severity: CRITICAL | HIGH | MEDIUM | LOW
  confidence: HIGH | MEDIUM | LOW
  title: String          // max 40 chars
  location: String       // shortest unique path + line
  issue: String          // one sentence
  fix: String            // actionable recommendation
  code_example?: String  // required for CRITICAL, optional for HIGH
}

fn gatherContext(target)       // Phase 1 â€” parse target, retrieve files, select perspectives
fn selectMode()                // Mode gate â€” AskUserQuestion for Standard vs Team
fn launchReviews(mode)         // Phase 2 â€” delegate to agents per mode
fn synthesize(findings)        // Phase 3 â€” dedup, rank, verdict, format
fn nextSteps(verdict)          // Phase 4 â€” AskUserQuestion based on verdict

## Constraints

Constraints {
  require {
    Launch ALL applicable review activities simultaneously in a single response.
    Every finding must have a specific, implementable fix.
    Describe what needs review; the system routes to specialists.
    Always highlight what's done well (strengths section).
    Provide full file context to reviewers, not just diffs.
    Only the lead's synthesized output is visible to the user; do not forward raw reviewer messages.
  }
  never {
    Review code yourself â€” always delegate to specialist agents.
    Present findings without actionable fix recommendations.
    Launch reviewers without full file context.
  }
}

## State

State {
  target = $ARGUMENTS
  perspectives = []              // populated by gatherContext
  mode: Standard | Team          // chosen by user in selectMode
  findings: [Finding]            // collected from agents in launchReviews
}

## Reference Materials

See `reference/` directory for detailed methodology:
- [Perspectives](reference/perspectives.md) â€” Perspective definitions, intent, activation rules
- [Output Format](reference/output-format.md) â€” Table guidelines, severity rules, verdict-based next steps
- [Output Example](examples/output-example.md) â€” Concrete example of expected output format
- [Checklist](reference/checklists.md) â€” Security, Performance, Quality, Test coverage checklists
- [Classification](reference/classification.md) â€” Severity/confidence definitions, classification matrix, example findings

## Workflow

fn gatherContext(target) {
  match (target) {
    /^\d+$/           => gh pr diff $target       // PR number
    "staged"          => git diff --cached        // staged changes
    containsSlash     => read file + recent changes  // file path
    default           => git diff main...$target  // branch name
  }

  Retrieve full file contents for context (not just diff).

  // Determine applicable conditional perspectives per reference/perspectives.md
  match (changes) {
    async/await | Promise | threading   => +Concurrency
    dependency file changes             => +Dependencies
    public API | schema changes         => +Compatibility
    frontend component changes          => +Accessibility
    CONSTITUTION.md exists              => +Constitution
  }
}

fn selectMode() {
  AskUserQuestion:
    Standard (default) â€” parallel fire-and-forget subagents
    Team Mode â€” persistent teammates with peer coordination

  Recommend Team Mode when:
    files > 10 | perspectives >= 4 | cross-domain | constitution active
}

fn launchReviews(mode) {
  match (mode) {
    Standard => launch parallel subagents per applicable perspectives
    Team     => create team, spawn one reviewer per perspective, assign tasks
  }
}

fn synthesize(findings) {
  findings
    |> deduplicate(groupBy: location, within: 5 lines, keep: highest severity, merge: complementary details)
    |> sort(by: [severity desc, confidence desc])
    |> assignIds(pattern: "$severityLetter$number")  // C1, C2, H1, M1, L1...
    |> buildSummaryTable

  verdict = match (criticalCount, highCount, mediumCount) {
    (> 0, _, _)     => ðŸ”´ REQUEST CHANGES
    (0, > 3, _)     => ðŸ”´ REQUEST CHANGES
    (0, 1..3, _)    => ðŸŸ¡ APPROVE WITH COMMENTS
    (0, 0, > 0)     => ðŸŸ¡ APPROVE WITH COMMENTS
    (0, 0, 0)       => âœ… APPROVE
  }

  Format report using template in reference/output-format.md.
}

fn nextSteps(verdict) {
  options = match (verdict) {
    ðŸ”´ REQUEST CHANGES       => loadOptions("request-changes", "reference/output-format.md")
    ðŸŸ¡ APPROVE WITH COMMENTS => loadOptions("approve-comments", "reference/output-format.md")
    âœ… APPROVE                => loadOptions("approve", "reference/output-format.md")
  }
  AskUserQuestion(options)
}

review(target) {
  gatherContext(target) |> selectMode |> launchReviews |> synthesize |> nextSteps
}
